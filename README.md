# ğŸš€ GPT-2 (124M) â€” Rebuilt from Scratch GPT-2

A **minimal**, **high-performance**, and **fully functional** reproduction of GPT-2 (124M parameters) implemented in **PyTorch**, inspired by Karpathyâ€™s *â€œLetâ€™s Build GPTâ€*.

Engineered with **modern optimizations**:
Flash Attention âš¡ | `torch.compile()` ğŸš€ | BF16 Mixed Precision ğŸ’¡ | Fused AdamW ğŸ”¥ | Efficient Sharded Data Loading ğŸ“¦

---

## ğŸŒŸ Why This Repo Exists

GPT-2 is still the **sweet spot** for:

* Learning LLM internals **without drowning in 1B+ parameters**
* Training on consumer hardware or Kaggle GPUs
* Experimenting with performance tricks used in modern GPT models

This codebase:

* **Faithfully reproduces GPT-2 architecture**
* **Trains faster** & **uses less memory**
* Enables **hands-on research** and **model extension**

---

## ğŸ§  Architecture & Implementation Choices

| Feature                      | What It Means                        | Why It Matters                                    |
| ---------------------------- | ------------------------------------ | ------------------------------------------------- |
| **Decoder-Only Transformer** | No encoder / cross-attention         | Standard for generative LLMs                      |
| **Pre-LayerNorm**            | Normalize before attention & MLP     | Better gradient flow, stabilizes deep networks    |
| **Weight Tying**             | Token embeddings = output projection | Fewer parameters, improved coherence              |
| **GPT-2 Init**               | Normal(0, 0.02)                      | Correct loss scaling at start of training (~10.8) |
| **Residual Scaling**         | Each block scaled by 1/âˆš(2L)         | Avoids variance blow-up                           |
| **Flash Attention**          | `scaled_dot_product_attention`       | Huge speed + memory savings                       |
| **torch.compile()**          | Kernel fusion & runtime optimization | Up to **2.3Ã— acceleration**                       |
| **BF16 Training**            | Autocast, no GradScaler              | Faster + cheaper training with stable numerics    |
| **Fused AdamW**              | Single-kernel optimizer              | Better GPU utilization                            |
| **Cosine LR + Warm-up**      | Proven stable LLM training schedule  | Prevents collapse early in training               |

> âš ï¸ First loss should â‰ˆ **10.8**
> (i.e., `-ln(1 / vocab_size)` with vocab=50257).
> Anything else? Somethingâ€™s wrong.

---

## ğŸ“‚ Project Structure

```
/
â”œâ”€â”€ train_gpt2.py      # GPT-2 model + training loop w/ optimizations
â”œâ”€â”€ fineweb.py         # Tokenize + shard FineWeb-Edu dataset
â”œâ”€â”€ out/               # Saved checkpoints
â”‚   â””â”€â”€ out_ckpt.pt    # Trained model weights (Git LFS)
â””â”€â”€ README.md          # You're reading it!
```

---

## âš™ï¸ Setup

### 1ï¸âƒ£ Install Dependencies

```bash
pip install torch numpy transformers datasets tiktoken tqdm requests
```

### 2ï¸âƒ£ Enable Git LFS

```bash
git lfs install
git lfs track "*.pt"
```

---

## ğŸ“š Dataset â€” FineWeb-Edu

A high-quality web-scale dataset curated for LLM training.

### Run Data Preprocessing

```bash
python fineweb.py
```

This creates a folder:

```
fineweb/
 â”œâ”€â”€ edufineweb_train_000001.bin
 â”œâ”€â”€ ...
```

> Each shard â‰ˆ 100M tokens â€” efficient streaming for long training runs.

---

## ğŸš‚ Training the Model

Default configuration:
**12 layers | 12 heads | 768 hidden dim | ~124M params**

### Standard Single-GPU Training

```bash
python train_gpt2.py
```

### Unlock Max Performance

(Toggle inside script or via CLI)

| Feature               | CLI Example            |
| --------------------- | ---------------------- |
| Compiled Model        | `--compile_model=True` |
| Mixed Precision BF16  | `--dtype=bfloat16`     |
| Gradient Accumulation | `--grad_accum_steps=8` |

### Multi-GPU Training (DDP)

```bash
torchrun --standalone --nproc_per_node=2 train_gpt2.py
```

DDP logic already handled â€” including
smart `require_backward_grad_sync` toggling during accumulation.

---

## ğŸ¤– Text Generation (Inference)

Example script logic:

```python
model = GPT2().from_checkpoint("out/out_ckpt.pt")
print(model.generate("Hello world,"))
```

> Use your trained model for **interactive generation demos**!

---

## ğŸ§ª Validation & Debugging

| Check                | Expected Value                         |
| -------------------- | -------------------------------------- |
| Initial Loss         | ~10.8                                  |
| FP16 instability?    | BF16 fixes it                          |
| Exploding gradients? | `grad_clip=1.0` included               |
| Bad generalization?  | Weight decay only on **2D parameters** |

---


## ğŸ—ï¸ Future Work Ideas

* âœ¨ Extend to GPT-2 Medium/XL
* ğŸ§© Add LoRA for cheap fine-tuning
* ğŸ“ˆ Integrate WandB logging
* ğŸŒ Train with rotary embeddings & sliding attention
* ğŸ’¬ Fine-tune on dialog datasets for chatbot usage

---

## ğŸ™ Acknowledgments

* **Andrej Karpathy** â€” nanoGPT architecture & awesome educational resources
* **OpenAI Authors** â€” GPT-2 architecture & original weights
* **Hugging Face** â€” FineWeb-Edu dataset + tokenizer tooling

---

## ğŸ“œ License

MIT â€” free to use, modify, and build upon.
